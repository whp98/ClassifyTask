# 本项目完成一个分类任务
给定数据，由于数据维度很高，分类过程中并不需要所有的特征，故可以对特征进行处理，选择对分类最有效的特征。

TODO——list

-[x] 针对数据实现一个分类算法

-[x] 前面的结果实现留一法交叉验证算法  

-[x] 给出特征选择（降维）后的分类误差（使用更少的有效特征进行分类）

-[x] 写作一个实验的报告，主要是报告实验过程

-[x] 提供原始代码/可执行的程序

## 数据降维算法
>主要的数据挖掘算法有
>
>>1.主成分分析算法（PCA）
>>
>>2.LDA
>>
>>3.局部线性嵌入 （LLE）
>>
>>4.Laplacian Eigenmaps 拉普拉斯特征映射
## 数据分类算法
>主要的数据分类算法
>
>决策树
>
>神经网络
>
>朴素贝叶斯和贝叶斯信念网络
>
>支持向量机
>
>Rule-based methods
>
>Distance-based methods
>
>Memory based reasoning

# 子任务实现

## 任务实现一：选择并实现一种分类算法应用于对给定的数据；
我们决定使用支持向量机来实现，分类算法,首先支持，首先我们进行数据预处理  
根据libsvm的格式存储要分类的数据，每种类型的数据使用20%作为验证数据集  
为了更好的分类我们先后使用了贝叶斯算法和支持向量机算法算法，在pac降维后我们得出支持向量机算法更加适合本实验，
因为在相同的维数下支持向量机得到了更好的结果 

## 任务实现二：使用全部特征进行分类计算出留一法交叉验证(love one out cross-valiation)的分类误差，并以此作为参考基值(baseline)；
这个过程比较简单，先将数据按照libsvm要求的格式存储到all.txt中然后使用自带的工具来进行归一化，使用svm读取label和特征。最后训练模型然后预测。  

对于每一个数据行所在的行，使用numpy的删除方法，得到除去验证数据的训练数据，使用剩余数据进行模型训练，然后等待训练结束后，使用svm模型对这一行数据进行预测，然后将该组数据的预测结果存储到一个list中。  
然后将list中的数据计算平均值，最后返回数据的误差的百分比数值。  

实现我们的分类算法，并开始准备验证最终验证结果是
```
误差值： 3.55329949238579
```
得出结论baseline=3.55329949238579

## 任务实现三：设计并实现一种特征选择方法或降维方法，给出特征选择（降维）后的分类误差（即使用更少的有效特征进行分类），结果必须比参考基值(baseline)好。


我们这里先后使用了方差降维和pca降维，经过实验验证pac降维效果更加好，能够在高准确的情况下将数据降低到个位数的维度，实在是太妙了所以，最终代码里面包含了方差降维，但是不会使用方差降维  
最终方案是SVM+PCA来解决问题

在上面的计算中我们已经得出了baseline=96.44670050761421我们决定在使用svm分类器的基础上使用PCA算法进行特征选取，进而提高分类的准确率，运行包含降维算法的代码后得出的结果如下  
```
误差值： 1.5228426395939039
```
这里将数据降到了六维，计算速度非常快，我们对准确率和计算结果都比较满意
###计算结果
|维数（pca降维）|留一交叉验证误差（%）|
|---|---|
|3312(原始)|3.55329949238579|
|降至100维 |7.614213197969548|
|降至50维| 5.583756345177662|
|降至10维|3.55329949238579|
|降至6维|1.5228426395939039|
## 任务实现四：写作一个实验的报告，主要是报告实验过程，参数，计算结果(形式自定，可以给出结果的图表)，并给出小组成员的姓名、学号和班级。

[实验报告](第13组实验报告.md)


